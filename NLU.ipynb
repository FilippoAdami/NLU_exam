{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2d9ef2beb0bf4a8a818f8a42e9a402b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_820bfb154e4e4b5eb60b8252c12c3f37",
              "IPY_MODEL_9a6988b455ef44368d516420c18042b7",
              "IPY_MODEL_f76726c0fdbf4f38ada624471d003463"
            ],
            "layout": "IPY_MODEL_beb937471f4a4329b2f08e87733acac3"
          }
        },
        "820bfb154e4e4b5eb60b8252c12c3f37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a13e01e972e447c2b40df821fe820568",
            "placeholder": "​",
            "style": "IPY_MODEL_e69843ca03d046dd8fda30f83733d666",
            "value": "PPL: 92.857124:  40%"
          }
        },
        "9a6988b455ef44368d516420c18042b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04f302800b83428586a08def3c385a27",
            "max": 99,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f7e04385e8249729520bac0a50e4fce",
            "value": 40
          }
        },
        "f76726c0fdbf4f38ada624471d003463": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12c3f09894c04d9fa31938ef703d970e",
            "placeholder": "​",
            "style": "IPY_MODEL_945f15237c234d5d8aa186003626e0df",
            "value": " 40/99 [36:58&lt;53:36, 54.52s/it]"
          }
        },
        "beb937471f4a4329b2f08e87733acac3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a13e01e972e447c2b40df821fe820568": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e69843ca03d046dd8fda30f83733d666": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04f302800b83428586a08def3c385a27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f7e04385e8249729520bac0a50e4fce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "12c3f09894c04d9fa31938ef703d970e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "945f15237c234d5d8aa186003626e0df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Colab setup\n",
        "!wget -P dataset/PennTreeBank https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.test.txt\n",
        "!wget -P dataset/PennTreeBank https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.valid.txt\n",
        "!wget -P dataset/PennTreeBank https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.train.txt\n",
        "\n",
        "# Imports\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from functools import partial\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bh3dnKBvv1v3",
        "outputId": "c95f6a40-7e49-457e-b8ff-6b3f21fdee43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-03 20:33:55--  https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 449945 (439K) [text/plain]\n",
            "Saving to: ‘dataset/PennTreeBank/ptb.test.txt’\n",
            "\n",
            "ptb.test.txt        100%[===================>] 439.40K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-11-03 20:33:55 (17.7 MB/s) - ‘dataset/PennTreeBank/ptb.test.txt’ saved [449945/449945]\n",
            "\n",
            "--2025-11-03 20:33:55--  https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.valid.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 399782 (390K) [text/plain]\n",
            "Saving to: ‘dataset/PennTreeBank/ptb.valid.txt’\n",
            "\n",
            "ptb.valid.txt       100%[===================>] 390.41K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-11-03 20:33:55 (19.0 MB/s) - ‘dataset/PennTreeBank/ptb.valid.txt’ saved [399782/399782]\n",
            "\n",
            "--2025-11-03 20:33:55--  https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5101618 (4.9M) [text/plain]\n",
            "Saving to: ‘dataset/PennTreeBank/ptb.train.txt’\n",
            "\n",
            "ptb.train.txt       100%[===================>]   4.87M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-11-03 20:33:56 (123 MB/s) - ‘dataset/PennTreeBank/ptb.train.txt’ saved [5101618/5101618]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "class LM_RNN(nn.Module):\n",
        "    def __init__(self, emb_size, hidden_size, output_size, pad_index=0, out_dropout=0.1,\n",
        "                 emb_dropout=0.1, n_layers=1):\n",
        "        super(LM_RNN, self).__init__()\n",
        "        # Token ids to vectors, we will better see this in the next lab\n",
        "        self.embedding = nn.Embedding(output_size, emb_size, padding_idx=pad_index)\n",
        "        # Pytorch's RNN layer: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
        "        self.rnn = nn.RNN(emb_size, hidden_size, n_layers, bidirectional=False, batch_first=True)\n",
        "        self.pad_token = pad_index\n",
        "        # Linear layer to project the hidden layer to our output space\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input_sequence):\n",
        "        emb = self.embedding(input_sequence)\n",
        "        rnn_out, _  = self.rnn(emb)\n",
        "        output = self.output(rnn_out).permute(0,2,1)\n",
        "        return output\n",
        "\n",
        "class LM_LSTM(nn.Module):\n",
        "    def __init__(self, emb_size, hidden_size, output_size, pad_index=0, out_dropout=0.1,\n",
        "                 emb_dropout=0.1, n_layers=1):\n",
        "        super(LM_LSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, emb_size, padding_idx=pad_index)\n",
        "        self.rnn = nn.LSTM(emb_size, hidden_size, n_layers, bidirectional=False, batch_first=True)  # LSTM instead of RNN\n",
        "        self.pad_token = pad_index\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "        self.output.weight = self.embedding.weight  # weight tying\n",
        "\n",
        "    def forward(self, input_sequence):\n",
        "        emb = self.embedding(input_sequence)\n",
        "        rnn_out, _ = self.rnn(emb)  # returns (output, (h_n, c_n))\n",
        "        output = self.output(rnn_out).permute(0, 2, 1)\n",
        "        return output\n",
        "\n",
        "class LM_LSTM_Dropout(nn.Module):\n",
        "    def __init__(self, emb_size, hidden_size, output_size, pad_index=0,\n",
        "                 emb_dropout=0.1, hid_dropout=0.1, n_layers=1):\n",
        "        super(LM_LSTM_Dropout, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, emb_size, padding_idx=pad_index)\n",
        "        self.emb_dropout = nn.Dropout(emb_dropout)          # Dropout after embeddings\n",
        "        self.rnn = nn.LSTM(emb_size, hidden_size, n_layers,\n",
        "                           batch_first=True, bidirectional=False)\n",
        "        self.out_dropout = nn.Dropout(hid_dropout)          # Dropout before final linear\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input_sequence):\n",
        "        emb = self.embedding(input_sequence)\n",
        "        emb = self.emb_dropout(emb)                         # Apply embedding dropout\n",
        "        rnn_out, _ = self.rnn(emb)\n",
        "        rnn_out = self.out_dropout(rnn_out)                # Apply output dropout\n",
        "        output = self.output(rnn_out).permute(0, 2, 1)\n",
        "        return output\n",
        "\n",
        "class LockedDropout(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LockedDropout, self).__init__()\n",
        "\n",
        "    def forward(self, x, dropout=0.5):\n",
        "        if not self.training or dropout == 0:\n",
        "            return x\n",
        "        mask = x.new_empty(1, x.size(1), x.size(2)).bernoulli_(1 - dropout)\n",
        "        mask = mask.div_(1 - dropout)\n",
        "        mask = mask.expand_as(x)\n",
        "        return x * mask\n",
        "\n",
        "class LM_LSTM_VDO(nn.Module):\n",
        "    def __init__(self, emb_size, hidden_size, output_size, pad_index,\n",
        "                 emb_dropout=0.1, hid_dropout=0.1, n_layers=1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_size, emb_size, padding_idx=pad_index)\n",
        "        self.lockdrop = LockedDropout()  # variational dropout\n",
        "        self.emb_dropout = emb_dropout\n",
        "        self.hid_dropout = hid_dropout\n",
        "\n",
        "        self.rnn = nn.LSTM(emb_size, hidden_size, n_layers,\n",
        "                           batch_first=True, bidirectional=False)\n",
        "\n",
        "        self.output = nn.Linear(hidden_size, output_size, bias=False)\n",
        "        self.output.weight = self.embedding.weight  # weight tying\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        emb = self.lockdrop(emb, self.emb_dropout)  # variational dropout on embeddings\n",
        "\n",
        "        rnn_out, _ = self.rnn(emb)\n",
        "        rnn_out = self.lockdrop(rnn_out, self.hid_dropout)  # variational dropout on hidden states\n",
        "\n",
        "        out = self.output(rnn_out).permute(0, 2, 1)\n",
        "        return out\n",
        "\n",
        "# Loading the corpus\n",
        "\n",
        "def read_file(path, eos_token=\"<eos>\"):\n",
        "    output = []\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f.readlines():\n",
        "            output.append(line.strip() + \" \" + eos_token)\n",
        "    return output\n",
        "\n",
        "# Vocab with tokens to ids\n",
        "def get_vocab(corpus, special_tokens=[]):\n",
        "    output = {}\n",
        "    i = 0\n",
        "    for st in special_tokens:\n",
        "        output[st] = i\n",
        "        i += 1\n",
        "    for sentence in corpus:\n",
        "        for w in sentence.split():\n",
        "            if w not in output:\n",
        "                output[w] = i\n",
        "                i += 1\n",
        "    return output\n",
        "\n",
        "# This class computes and stores our vocab\n",
        "# Word to ids and ids to word\n",
        "class Lang():\n",
        "    def __init__(self, corpus, special_tokens=[]):\n",
        "        self.word2id = self.get_vocab(corpus, special_tokens)\n",
        "        self.id2word = {v:k for k, v in self.word2id.items()}\n",
        "    def get_vocab(self, corpus, special_tokens=[]):\n",
        "        output = {}\n",
        "        i = 0\n",
        "        for st in special_tokens:\n",
        "            output[st] = i\n",
        "            i += 1\n",
        "        for sentence in corpus:\n",
        "            for w in sentence.split():\n",
        "                if w not in output:\n",
        "                    output[w] = i\n",
        "                    i += 1\n",
        "        return output\n",
        "\n",
        "class PennTreeBank (data.Dataset):\n",
        "    # Mandatory methods are __init__, __len__ and __getitem__\n",
        "    def __init__(self, corpus, lang):\n",
        "        self.source = []\n",
        "        self.target = []\n",
        "\n",
        "        for sentence in corpus:\n",
        "            self.source.append(sentence.split()[0:-1]) # We get from the first token till the second-last token\n",
        "            self.target.append(sentence.split()[1:]) # We get from the second token till the last token\n",
        "            # See example in section 6.2\n",
        "\n",
        "        self.source_ids = self.mapping_seq(self.source, lang)\n",
        "        self.target_ids = self.mapping_seq(self.target, lang)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.source)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src= torch.LongTensor(self.source_ids[idx])\n",
        "        trg = torch.LongTensor(self.target_ids[idx])\n",
        "        sample = {'source': src, 'target': trg}\n",
        "        return sample\n",
        "\n",
        "    # Auxiliary methods\n",
        "\n",
        "    def mapping_seq(self, data, lang): # Map sequences of tokens to corresponding computed in Lang class\n",
        "        res = []\n",
        "        for seq in data:\n",
        "            tmp_seq = []\n",
        "            for x in seq:\n",
        "                if x in lang.word2id:\n",
        "                    tmp_seq.append(lang.word2id[x])\n",
        "                else:\n",
        "                    print('OOV found!')\n",
        "                    print('You have to deal with that') # PennTreeBank doesn't have OOV but \"Trust is good, control is better!\"\n",
        "                    break\n",
        "            res.append(tmp_seq)\n",
        "        return res\n",
        "\n",
        "def collate_fn(data, pad_token):\n",
        "    def merge(sequences):\n",
        "        '''\n",
        "        merge from batch * sent_len to batch * max_len\n",
        "        '''\n",
        "        lengths = [len(seq) for seq in sequences]\n",
        "        max_len = 1 if max(lengths)==0 else max(lengths)\n",
        "        # Pad token is zero in our case\n",
        "        # So we create a matrix full of PAD_TOKEN (i.e. 0) with the shape\n",
        "        # batch_size X maximum length of a sequence\n",
        "        padded_seqs = torch.LongTensor(len(sequences),max_len).fill_(pad_token)\n",
        "        for i, seq in enumerate(sequences):\n",
        "            end = lengths[i]\n",
        "            padded_seqs[i, :end] = seq # We copy each sequence into the matrix\n",
        "        padded_seqs = padded_seqs.detach()  # We remove these tensors from the computational graph\n",
        "        return padded_seqs, lengths\n",
        "\n",
        "    # Sort data by seq lengths\n",
        "    data.sort(key=lambda x: len(x[\"source\"]), reverse=True)\n",
        "    new_item = {}\n",
        "    for key in data[0].keys():\n",
        "        new_item[key] = [d[key] for d in data]\n",
        "\n",
        "    source, _ = merge(new_item[\"source\"])\n",
        "    target, lengths = merge(new_item[\"target\"])\n",
        "\n",
        "    new_item[\"source\"] = source.to(DEVICE)\n",
        "    new_item[\"target\"] = target.to(DEVICE)\n",
        "    new_item[\"number_tokens\"] = sum(lengths)\n",
        "    return new_item\n",
        "\n",
        "def train_loop(data, optimizer, criterion, model, clip=5):\n",
        "    model.train()\n",
        "    loss_array = []\n",
        "    number_of_tokens = []\n",
        "\n",
        "    for sample in data:\n",
        "        optimizer.zero_grad() # Zeroing the gradient\n",
        "        output = model(sample['source'])\n",
        "        loss = criterion(output, sample['target'])\n",
        "        loss_array.append(loss.item() * sample[\"number_tokens\"])\n",
        "        number_of_tokens.append(sample[\"number_tokens\"])\n",
        "        loss.backward() # Compute the gradient, deleting the computational graph\n",
        "        # clip the gradient to avoid exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step() # Update the weights\n",
        "\n",
        "    return sum(loss_array)/sum(number_of_tokens)\n",
        "\n",
        "def eval_loop(data, eval_criterion, model):\n",
        "    model.eval()\n",
        "    loss_to_return = []\n",
        "    loss_array = []\n",
        "    number_of_tokens = []\n",
        "    # softmax = nn.Softmax(dim=1) # Use Softmax if you need the actual probability\n",
        "    with torch.no_grad(): # It used to avoid the creation of computational graph\n",
        "        for sample in data:\n",
        "            output = model(sample['source'])\n",
        "            loss = eval_criterion(output, sample['target'])\n",
        "            loss_array.append(loss.item())\n",
        "            number_of_tokens.append(sample[\"number_tokens\"])\n",
        "\n",
        "    ppl = math.exp(sum(loss_array) / sum(number_of_tokens))\n",
        "    loss_to_return = sum(loss_array) / sum(number_of_tokens)\n",
        "    return ppl, loss_to_return\n",
        "\n",
        "def init_weights(mat):\n",
        "    for m in mat.modules():\n",
        "        if type(m) in [nn.GRU, nn.LSTM, nn.RNN]:\n",
        "            for name, param in m.named_parameters():\n",
        "                if 'weight_ih' in name:\n",
        "                    for idx in range(4):\n",
        "                        mul = param.shape[0]//4\n",
        "                        torch.nn.init.xavier_uniform_(param[idx*mul:(idx+1)*mul])\n",
        "                elif 'weight_hh' in name:\n",
        "                    for idx in range(4):\n",
        "                        mul = param.shape[0]//4\n",
        "                        torch.nn.init.orthogonal_(param[idx*mul:(idx+1)*mul])\n",
        "                elif 'bias' in name:\n",
        "                    param.data.fill_(0)\n",
        "        else:\n",
        "            if type(m) in [nn.Linear]:\n",
        "                torch.nn.init.uniform_(m.weight, -0.01, 0.01)\n",
        "                if m.bias != None:\n",
        "                    m.bias.data.fill_(0.01)\n",
        "\n",
        "def load_asgd_averaged_weights(model, optimizer):\n",
        "    for group in optimizer.param_groups:\n",
        "        for p in group['params']:\n",
        "            state = optimizer.state[p]\n",
        "            if 'ax' in state and state['ax'] is not None:\n",
        "                p.data.copy_(state['ax'])\n",
        "\n",
        "\n",
        "\n",
        "# 'TUNABLE' PARAMETERS\n",
        "DEVICE = 'cuda:0'\n",
        "TBS = 32\n",
        "LR=3\n",
        "HID_DIM=700\n",
        "EMB_SIZE=700\n",
        "n_epochs = 100\n",
        "EMB_DO=0.4\n",
        "OUT_DO=0.7\n",
        "\n",
        "train_raw = read_file(\"dataset/PennTreeBank/ptb.train.txt\")\n",
        "dev_raw = read_file(\"dataset/PennTreeBank/ptb.valid.txt\")\n",
        "test_raw = read_file(\"dataset/PennTreeBank/ptb.test.txt\")\n",
        "\n",
        "clip = 5 # Clip the gradient\n",
        "vocab = get_vocab(train_raw, [\"<pad>\", \"<eos>\"])\n",
        "lang = Lang(train_raw, [\"<pad>\", \"<eos>\"])\n",
        "vocab_len = len(lang.word2id)\n",
        "#MODEL= LM_RNN(EMB_SIZE, HID_DIM, vocab_len, pad_index=lang.word2id[\"<pad>\"]).to(DEVICE) # RNN MODEL\n",
        "#MODEL = LM_LSTM(EMB_SIZE, HID_DIM, vocab_len, pad_index=lang.word2id[\"<pad>\"]).to(DEVICE) # LSTM MODEL (with or without weight tying)\n",
        "#MODEL = LM_LSTM_Dropout(EMB_SIZE, HID_DIM, vocab_len,pad_index=lang.word2id[\"<pad>\"], emb_dropout=EMB_DO, hid_dropout=OUT_DO).to(DEVICE) # LSTM with DropOut MODEL\n",
        "MODEL = LM_LSTM_VDO(EMB_SIZE, HID_DIM, vocab_len,pad_index=lang.word2id[\"<pad>\"], emb_dropout=EMB_DO, hid_dropout=OUT_DO).to(DEVICE) # LSTM with Variational DropOut MODEL\n",
        "model = MODEL\n",
        "model.apply(init_weights)\n",
        "\n",
        "#optimizer = optim.SGD(model.parameters(), lr=LR)  # SDG OPTIMIZER\n",
        "#optimizer = torch.optim.AdamW(MODEL.parameters(), lr=LR, weight_decay=1e-2) # ADAMW OPTIMIZER\n",
        "optimizer = torch.optim.ASGD(\n",
        "    model.parameters(),\n",
        "    lr=LR,\n",
        "    t0=0,        # 0 = no averaging initially\n",
        "    lambd=0.0,   # disable weight decay here, keep explicit reg if needed\n",
        "    alpha=0.75   # ASGD averaging power (default works fine)\n",
        ")\n",
        "\n",
        "criterion_train = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"])\n",
        "criterion_eval = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"], reduction='sum')\n",
        "\n",
        "train_dataset = PennTreeBank(train_raw, lang)\n",
        "dev_dataset = PennTreeBank(dev_raw, lang)\n",
        "test_dataset = PennTreeBank(test_raw, lang)\n",
        "\n",
        "# Main loop\n",
        "train_loader = DataLoader(train_dataset, batch_size=TBS, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]),  shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=128, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))\n",
        "patience = 3\n",
        "losses_train = []\n",
        "losses_dev = []\n",
        "sampled_epochs = []\n",
        "best_ppl = math.inf\n",
        "best_model = None\n",
        "triggered = False  # flag for NT-ASGD averaging\n",
        "\n",
        "# Store averaged params outside optimizer.state to avoid PyTorch foreach grouping issues\n",
        "avg_params = {}  # maps id(param) -> tensor(cpu)\n",
        "\n",
        "pbar = tqdm(range(1, n_epochs))\n",
        "for epoch in pbar:\n",
        "    loss = train_loop(train_loader, optimizer, criterion_train, model, clip)\n",
        "\n",
        "    if epoch % 1 == 0:\n",
        "        sampled_epochs.append(epoch)\n",
        "        losses_train.append(np.asarray(loss).mean())\n",
        "        ppl_dev, loss_dev = eval_loop(dev_loader, criterion_eval, model)\n",
        "        losses_dev.append(np.asarray(loss_dev).mean())\n",
        "        pbar.set_description(\"PPL: %f\" % ppl_dev)\n",
        "\n",
        "        if ppl_dev < best_ppl:  # improvement\n",
        "            best_ppl = ppl_dev\n",
        "            best_model = copy.deepcopy(model).to('cpu')\n",
        "            patience = 3\n",
        "        else:\n",
        "            patience -= 1\n",
        "\n",
        "        # --- NT-ASGD trigger (SAFE: keep averages outside optimizer.state) ---\n",
        "        if patience <= 0 and not triggered:\n",
        "            print(\">>> Triggering NT-ASGD averaging (safe mode)\")\n",
        "\n",
        "            # initialize averaged params in a separate dict (move to CPU to save GPU memory)\n",
        "            for group in optimizer.param_groups:\n",
        "                for p in group['params']:\n",
        "                    avg_params[id(p)] = p.detach().cpu().clone()  # store CPU copy\n",
        "                # ↓ reduce LR after averaging starts\n",
        "                group['lr'] *= 0.33\n",
        "\n",
        "            # If you want to keep t0 metadata, store it in param_group dict (optional)\n",
        "            optimizer.param_groups[0]['t0'] = epoch\n",
        "            triggered = True\n",
        "            patience = 3  # reset patience to allow averaging to continue\n",
        "\n",
        "\n",
        "        if patience <= 0 and triggered:  # early stop after averaging\n",
        "            break\n",
        "if triggered:\n",
        "    print(\">>> Loading NT-ASGD averaged weights into model (from avg_params)\")\n",
        "    # Copy averaged weights (stored on CPU) back into model parameters (on DEVICE)\n",
        "    for group in optimizer.param_groups:\n",
        "        for p in group['params']:\n",
        "            key = id(p)\n",
        "            if key in avg_params:\n",
        "                p.data.copy_(avg_params[key].to(p.device))\n",
        "\n",
        "\n",
        "best_model.to(DEVICE)\n",
        "final_ppl,  _ = eval_loop(test_loader, criterion_eval, best_model)\n",
        "print('Test ppl: ', final_ppl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "2d9ef2beb0bf4a8a818f8a42e9a402b4",
            "820bfb154e4e4b5eb60b8252c12c3f37",
            "9a6988b455ef44368d516420c18042b7",
            "f76726c0fdbf4f38ada624471d003463",
            "beb937471f4a4329b2f08e87733acac3",
            "a13e01e972e447c2b40df821fe820568",
            "e69843ca03d046dd8fda30f83733d666",
            "04f302800b83428586a08def3c385a27",
            "7f7e04385e8249729520bac0a50e4fce",
            "12c3f09894c04d9fa31938ef703d970e",
            "945f15237c234d5d8aa186003626e0df"
          ]
        },
        "id": "4xC6Pu7jv3su",
        "outputId": "4093233c-2592-4f91-9581-5a031e558726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/99 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d9ef2beb0bf4a8a818f8a42e9a402b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Triggering NT-ASGD averaging (safe mode)\n",
            ">>> Loading NT-ASGD averaged weights into model (from avg_params)\n",
            "Test ppl:  89.84049225798147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To save the model\n",
        "path = '/model_LSTM_NTAvSDG.pt'\n",
        "torch.save(model.state_dict(), path)\n",
        "# To load the model you need to initialize it\n",
        "# model = LM_RNN(emb_size, hid_size, vocab_len, pad_index=lang.word2id[\"<pad>\"]).to(device)\n",
        "# Then you load it\n",
        "# model.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "id": "VSnWfitcv4ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Rebuild the model architecture ---\n",
        "loaded_model = LM_LSTM_VDO(\n",
        "    EMB_SIZE,\n",
        "    HID_DIM,\n",
        "    vocab_len,\n",
        "    pad_index=lang.word2id[\"<pad>\"],\n",
        "    emb_dropout=EMB_DO,\n",
        "    hid_dropout=OUT_DO\n",
        ").to(DEVICE)\n",
        "\n",
        "# --- Load weights ---\n",
        "loaded_model.load_state_dict(torch.load(\"/model_LSTM_NTAvSDG.pt\", map_location=DEVICE))\n",
        "\n",
        "# --- Evaluate on test set ---\n",
        "loaded_model.eval()\n",
        "final_ppl, final_loss = eval_loop(test_loader, criterion_eval, loaded_model)\n",
        "\n",
        "print(\"Loaded model test PPL:\", final_ppl)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f98Tw8U28jN_",
        "outputId": "821f8e33-3211-4b98-881a-67da0e00cb02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model test PPL: 92.1442226468789\n"
          ]
        }
      ]
    }
  ]
}